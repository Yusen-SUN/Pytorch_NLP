{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from word2vector import Vocab\n",
    "from EncoderRNN import EncoderRNN\n",
    "from Attention import Attn\n",
    "from DecoderRNN import DecoderRNN\n",
    "from Data_reading import input_data\n",
    "from Pretrained_embedding import pre_embedding\n",
    "import Train\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_train (98634, 40)\n",
      "src_train_lens (98634,)\n",
      "tgt_train (98634, 40)\n",
      "tgt_train_lens (98634,)\n",
      "src_val (4962, 40)\n",
      "src_val_lens (4962,)\n",
      "tgt_val (4962, 40)\n",
      "tgt_val_lens (4962,)\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 40\n",
    "src_train, src_train_lens, tgt_train, tgt_train_lens, src_val, src_val_lens, tgt_val, tgt_val_lens =  input_data(sp_user, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(src_train_lens, np.max(src_train_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(tgt_train_lens[tgt_train_lens<100], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(src_val_lens, np.max(src_val_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(tgt_val_lens, np.max(tgt_val_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.arange(5000)\n",
    "formal_train_tensors = torch.LongTensor(src_train[test]).to(device)\n",
    "formal_train_lens_tensors = torch.LongTensor(src_train_lens[test]).to(device)\n",
    "informal_train_tensors = torch.LongTensor(tgt_train[test]).to(device)\n",
    "informal_train_lens_tensors = torch.LongTensor(tgt_train_lens[test]).to(device)\n",
    "\n",
    "test_1= np.arange(1000)\n",
    "formal_val_tensors = torch.LongTensor(src_val[test_1]).to(device)\n",
    "formal_val_lens_tensors = torch.LongTensor(src_val_lens[test_1]).to(device)\n",
    "informal_val_tensors = torch.LongTensor(tgt_val[test_1]).to(device)\n",
    "informal_val_lens_tensors = torch.LongTensor(tgt_val_lens[test_1]).to(device)\n",
    "\n",
    "# formal_train_tensors = torch.LongTensor(src_train).to(device)\n",
    "# formal_train_lens_tensors = torch.LongTensor(src_train_lens).to(device)\n",
    "# informal_train_tensors = torch.LongTensor(tgt_train).to(device)\n",
    "# informal_train_lens_tensors = torch.LongTensor(tgt_train_lens).to(device)\n",
    "\n",
    "# formal_val_tensors = torch.LongTensor(src_val).to(device)\n",
    "# formal_val_lens_tensors = torch.LongTensor(src_val_lens).to(device)\n",
    "# informal_val_tensors = torch.LongTensor(tgt_val).to(device)\n",
    "# informal_val_lens_tensors = torch.LongTensor(tgt_val_lens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_layer = 4\n",
    "dropout = 0.5\n",
    "hidden_dim = 128\n",
    "batch_size = 64\n",
    "learning_rate=0.001\n",
    "vocab_size = sp_user.get_piece_size() #len(Glove.word2index)\n",
    "#embedding=None\n",
    "attn = 'general'\n",
    "style_1 = sp_user.piece_to_id('<informal>')\n",
    "style_2 = sp_user.piece_to_id('<formal>')\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "\n",
    "encoder = EncoderRNN(embedding=embedding, vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, device=device ,\n",
    "                     num_layer=num_layer, dropout=dropout).to(device)\n",
    "\n",
    "decoder = DecoderRNN(embedding=embedding, attn_model=attn, vocab_size=vocab_size, embedding_dim=embedding_dim, \n",
    "                     hidden_dim=hidden_dim, device=device , num_layer=num_layer, dropout=dropout).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(),lr=learning_rate)\n",
    "\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333337\n",
      "iter 1\n",
      "4992/5000, loss:16.686233520507812\n",
      "Evalidation: 960/1000, loss:15.162066650390624\n",
      "women are acquisitive . give them both and they will say , \" it is not working well enough , darling .\"\n",
      "i i i i\n",
      "i i i i\n",
      "women are greedy , give them the both , and they will say , it isnot working good enough , honey\n",
      "i i i i\n",
      "i i i i\n",
      "\n",
      "0m 45s (- 1m 30s) train loss: 16.046.  val loss: 13.735.\n",
      "val loss decreases from 100.000 to 13.735, save models\n",
      "\n",
      "-0.33333333333333326\n",
      "iter 2\n",
      "4992/5000, loss:17.27268409729004\n",
      "Evalidation: 960/1000, loss:14.4638916015625\n",
      "women are acquisitive . give them both and they will say , \" it is not working well enough , darling .\"\n",
      "i i i i i i i , , , , , , , , , ,\n",
      "i i i i i i i , , , , , , , , , ,\n",
      "women are greedy , give them the both , and they will say , it isnot working good enough , honey\n",
      "i i i i i i i , , , , , , , , , ,\n",
      "i i i i i i i , , , , , , , , , ,\n",
      "\n",
      "1m 31s (- 0m 45s) train loss: 12.111.  val loss: 13.231.\n",
      "val loss decreases from 13.735 to 13.231, save models\n",
      "\n",
      "-1.0\n",
      "iter 3\n",
      "4608/5000, loss:13.001825332641602"
     ]
    }
   ],
   "source": [
    "output_seq, plot_train_loss, plot_val_loss = Train.train(formal_train_tensors, formal_train_lens_tensors, informal_train_tensors, informal_train_lens_tensors,\n",
    "            formal_val_tensors, formal_val_lens_tensors, informal_val_tensors, informal_val_lens_tensors,\n",
    "            style_1=style_1, style_2=style_2,\n",
    "            encoder=encoder, decoder=decoder, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, \n",
    "            epoches=3, batch_size=batch_size, print_every=1, plot_every=1, sp_user=sp_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_seq[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6):\n",
    "#     print(sp_user.decode_ids(output_seq[5][i].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./saved_models/embedding', 'w') as f:\n",
    "    print(embedding.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script unsupervised.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
